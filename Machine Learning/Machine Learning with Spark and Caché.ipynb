{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Spark and Caché\n",
    "\n",
    "[Apache Spark](http://spark.apache.org/) has rapidly become one of the most exciting technologies for big data analytics and machine learning. Spark is a general data processing engine created for use in clustered computing environments. It's heart is the Resilient Distributed Dataset (RDD) which represents a distributed, fault tolerant, collection of data that can be operated on in parallel across the nodes of a cluster. Spark is implemented using a combination of Java and Scala and so comes as a library that can run on any JVM. Spark also supports Python (PySpark) and R (SparkR) and includes libraries for SQL (SparkSQL), machine learning (MLlib), graph processing (GraphX), and stream processing (Spark Streaming). \n",
    "\n",
    "When the Cach&eacute; Spark connector is released, Spark-based applications will be able to make full use of Caché as an open analytics platform. That means taking full advantage of the capabilities of the underlying Cach&eacute; database by optimizing throughput through parallelization and pushing down appropriate filtering work to the database, minimizing the amount of data that needs to be read. However, today using a plain JDBC connection to Cach&eacute;, we can already begin using Spark with Cach&eacute; and then transparently upgrade to the new connector when it becomes available.\n",
    "\n",
    "The following is my attempt to demonstrate a machine learning &quot;Hello World&quot; using Spark and Cach&eacute; running locally on my laptop. I show a couple of machine learning examples (linear regression and naive Bayes classification) using PySpark and a JDBC connection to  Cach&eacute;. Conveniently, Cach&eacute;&apos;s SAMPLES namespace contains a copy of the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), one of the classics for machine learning demonstrations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Spark and Cach&eacute; on my Windows laptop\n",
    "To install Spark locally on my laptop, I essentially followed the steps outlined here: [How to run Apache Spark on Windows 7 in Standalone Mode](http://nishutayaltech.blogspot.in/2015/04/how-to-run-apache-spark-on-windows7-in.html)\n",
    "Here are some details:\n",
    "* Java 1.8\n",
    "* Note: I did not install Scala separately as described in the above article. The pre-built Spark libraries I installed already contained the Scala executables and jars.\n",
    "* Python 2.7.12. I installed this as part of the Anaconda 4.2 package.\n",
    "* Spark version 2.6. Prebuilt package for Hadoop.\n",
    "* findspark python package. I installed this using pip install findspark. This package uses the SPARK_HOME environment variable   to locate the Spark installation. This makes it easier to import Spark into python code.\n",
    "* Environment variables:\n",
    "    * SPARK_HOME: pointing to the spark installation directory.\n",
    "    * HADOOP_HOME: pointing to the Hadoop installation directory. Note that winutils.exe must be installed in the bin \n",
    "      subdirectory. The above article discusses this dependency and provides a link for downloading the file.\n",
    "* Directories: c:\\spark\\temp\n",
    "      \n",
    "Here is my Cach&eacute; setup:\n",
    "* Cach&eacute; 2016.2.1\n",
    "* I configured the following environment variable so that python would be able to find the Cach&eacute; jdbc driver:\n",
    "    * SPARK_CLASSPATH: cache-install-dir\\dev\\lib\\java\\lib\\jdk18\\cachejdbc.jar\n",
    "*  To prepare the Iris dataset, I ran the following Cach&eacute; command in the SAMPLES namespace:\n",
    "  ```\n",
    "  SAMPLES>Do ##class(DataMining.IrisDataset).load()\n",
    "  ``` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can execute the following code samples from the command line. Open your [spark installation]/bin directory and enter &quot;pyspark&quot;. You should see output like the following:\n",
    "<img src=\"pyspark.png\">\n",
    "\n",
    "\n",
    "I wrote this entire article, inlcuding all the code samples, in a jupyter notebook. You can get this on GitHub [Machine Learning with Spark and Cach&eacute;](https://github.com/denelson/DevCommunity/tree/master/Machine%20Learning). Once your environment is ready you can launch the notebook and execute all the code samples directly from within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll use findspark to do a quick test to verify that Spark is correctly configured and that we can import it into our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "# If the Spark context was created, we should see output that looks something like <pyspark.context.SparkContext at 0x2b2aeb8>\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Examining Some Data\n",
    "Next we will create a SparkSession instance and use it to connect to Cach&eacute;. SparkSession is the starting point for using Spark. We will use it to load the Iris dataset into a Spark DataFrame. The Spark DataFrame extends the functionality of the original Spark RDD (discussed above). In addition to many optimizations, the DataFrame adds the ability to access and manipulate data through both a relational sql-like interface and a list of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\")\n",
    "spark = SparkSession.builder.config('spark.sql.warehouse.dir','file:///C:/spark/temp')\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "iris = spark.read.format(\"jdbc\").option(\"url\",\"jdbc:Cache://localhost:1972/SAMPLES\").option(\"driver\",\n",
    "\"com.intersys.jdbc.CacheDriver\").option(\"dbtable\",\"DataMining.IrisDataset\").option(\"user\",\n",
    "\"_System\").option(\"password\",\"SYS\").option(\"spark.sql.warehouse.dir\",\"file:///C:/spark/temp\").load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can run a command to display the first 10 rows of Iris data as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, a sepal is a leaf, usually green, that serves to protect a flower in its bud stage and then physically support the flower when it blooms.\n",
    "\n",
    "We can do a variety of SQL-like operations, for example finding the number rows where PetalLength is greater than 6.0 or finding the counts of the different species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.filter(iris[\"PetalLength\"]>6.0).show()\n",
    "iris.groupBy(\"Species\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the first 10 rows displayed as a python list of Spark Row objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code acccesses the Iris data through the list interface to create a pair of arrays to use with the matplotlib charting library. Spark unfortunately does not have its own charting library. The code creates a scatter plot showing PetalLength vs. PetalWidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Retrieve an array of row objects from the DataFrame\n",
    "items = iris.collect()\n",
    "petal_length = []\n",
    "petal_width = []\n",
    "for item in items:\n",
    "    petal_length.append(item['PetalLength'])\n",
    "    petal_width.append(item['PetalWidth'])\n",
    "\n",
    "plt.scatter(petal_width,petal_length)\n",
    "plt.xlabel(\"Petal Width\")\n",
    "plt.ylabel(\"Petal Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing a Linear Regression Model\n",
    "Looks like there is a pretty strong linear relationship between PetalWidth and PetalLength. I suppose that&apos;s not surprising. Let&apos;s investigate the relationship more closely using Spark's machine learning library. We will train a simple linear regression model to fit a line through the data. Once we have the model we can use it to predict the length of an Iris petal based on its width.\n",
    "\n",
    "Here is an outline of the steps in the following code:\n",
    "\n",
    "1. Create a new DataFrame and transform the PetalWidth or \"features\" column into the vector needed by the Spark library.\n",
    "2. Randomly divide the Iris data into training (70%) and test (30%) sets.\n",
    "3. Use the training data to fit a linear regression model, the actual machine learning.\n",
    "4. Run the test data through the model and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Transform the \"Features\" column(s) into the correct vector format\n",
    "df = iris.select('PetalLength','PetalWidth')\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"PetalWidth\"],\n",
    "                                  outputCol=\"features\")\n",
    "data=vectorAssembler.transform(df)   \n",
    "\n",
    "# Split the data into training and test sets.\n",
    "trainingData,testData = data.randomSplit([0.7, 0.3], 0.0)\n",
    "\n",
    "# Configure the model.\n",
    "lr = LinearRegression().setFeaturesCol(\"features\").setLabelCol(\"PetalLength\").setMaxIter(10)\n",
    "\n",
    "# Train the model using the training data.\n",
    "lrm = lr.fit(trainingData)\n",
    "\n",
    "# Run the test data through the model and display its predictions for PetalLength.\n",
    "predictions = lrm.transform(testData)\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction column shows the petal length predicted by the model. We can compare it to the actual values in the PetalLength column. \n",
    "\n",
    "The next piece of code evaluates the model by calculating the root mean squared error (RMSE) for its predictions on the test data. This provides one measure of the model&apos;s accuracy. The code also retrieves the slope and y-intercept of the regression line. We will use this to add the regression line to our earlier scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# retrieve the slope and y-intercepts of the regression line from the model.\n",
    "slope = lrm.coefficients[0]\n",
    "intercept = lrm.intercept\n",
    "\n",
    "print(\"slope of regression line: %s\" % str(slope))\n",
    "print(\"y-intercept of regression line: %s\" % str(intercept))\n",
    "\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"PetalLength\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this RMSE value it is not perfectly clear to me how well our model does at predicting petal length. We can compare the error to the average PetalLength value and perhaps get some sense of the error&apos;s significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris.describe([\"PetalLength\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let&apos;s visualize the model by adding the regression line determined by the above slope and intercept to our original scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "items = iris.collect()\n",
    "petal_length = []\n",
    "petal_width = []\n",
    "petal_features = []\n",
    "for item in items:\n",
    "    petal_length.append(item['PetalLength'])\n",
    "    petal_width.append(item['PetalWidth'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(petal_width,petal_length)\n",
    "plt.xlabel(\"Petal Width\")\n",
    "plt.ylabel(\"Petal Length\")\n",
    "y = [slope*x+intercept for x in petal_width]\n",
    "ax.plot(petal_width, y, color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing a Classification Model\n",
    "The Iris data contains three different species of Iris: Iris-Setosa, Iris-Verisicolor, and Iris-Virginica. We can train a model to classify or predict which species a flower belongs to based on its features: PetalLength, PetalWidth, SepalLength, and SepalWidth. Spark supports several different classification algorithms. The following code uses the Naive Bayes algorithm, one of the simpler yet still very powerful classification algorithms.\n",
    "\n",
    "Here is an outline of the steps:\n",
    "1. Prepare the data for the model. This involves putting the features into a vector. It also involves indexing the classes, replacing \"Iris-Setosa\" with 0.0, \"Iris-verisicolor\" with 1.0, and \"Iris-Virginica\" with 2.0.\n",
    "2. Randomly divide the Iris data into training (70%) and test (30%) sets.\n",
    "3. Train the classifier on the training data.\n",
    "4. Run the test data through the model to generate predicted classifications\n",
    "5. Un-index the predictions so we can see the species names rather than the indexes in the output.\n",
    "6. Display the actual and predicted species side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString\n",
    "\n",
    "# Prepare the data by indexing the classes and putting the features into a vector.\n",
    "speciesIndexer = StringIndexer(inputCol=\"Species\", outputCol=\"speciesIndex\")\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"PetalWidth\",\"PetalLength\",\"SepalWidth\",\"SepalLength\"],\n",
    "                                  outputCol=\"features\")\n",
    "\n",
    "data = vectorAssembler.transform(iris)\n",
    "index_model = speciesIndexer.fit(data)\n",
    "data_indexed = index_model.transform(data)\n",
    "\n",
    "# Split the data into training and test sets.\n",
    "trainingData, testData =  data_indexed.randomSplit([0.7, 0.3],0.0)\n",
    "\n",
    "# Configure the classifier and then train it using the training set.\n",
    "nb = NaiveBayes().setFeaturesCol(\"features\").setLabelCol(\"speciesIndex\").setSmoothing(1.0).setModelType(\"multinomial\")\n",
    "model = nb.fit(trainingData)\n",
    "\n",
    "# Run the classifier on the test set\n",
    "classifications = model.transform(testData)\n",
    "\n",
    "# Un-index the data so we have the species names rather than the index numbers in our output.\n",
    "converter = IndexToString(inputCol=\"prediction\", outputCol=\"PredictedSpecies\", labels=index_model.labels)\n",
    "converted = converter.transform(classifications)\n",
    "\n",
    "# Display the actual and predicted species side-by-side\n",
    "converted.select(['Species','PredictedSpecies']).show(45)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the classifier was not perfect. In the above subset of the data it misclassified two of the Iris-Verisicolor and one of the Iris-Virginica's. We can use an evaluator to calculate the exact accuracy of the classifier on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"speciesIndex\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(classifications)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If this accuracy is not sufficient, we could tune some parameters of the model or even try an entirely different classification algorithm."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
